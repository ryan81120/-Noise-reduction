{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation, SpatialDropout2D, Reshape, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU, PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import layers\n",
    "from keras.layers import LSTM ,Embedding ,SimpleRNN,CuDNNLSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from scipy.io import wavfile\n",
    "import pdb\n",
    "import scipy.io\n",
    "# import librosa\n",
    "import os\n",
    "from os.path import join as ojoin\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import time  \n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import argparse\n",
    "import random\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "#設定使用GPU 檢查是否有抓到\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.99 #使用45%記憶體\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義讀檔路徑function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(directory):\n",
    "    \"\"\"\n",
    "    This function will generate the file names in a directory \n",
    "    tree by walking the tree either top-down or bottom-up. For each \n",
    "    directory in the tree rooted at directory top (including top itself), \n",
    "    it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "    \"\"\"\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "\n",
    "    # Walk the tree.\n",
    "\n",
    "    for root, directories, files in os.walk(directory):\n",
    "\n",
    "        for filename in files:\n",
    "            if filename.endswith('.wav'):\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "                filepath = os.path.join(root, filename)\n",
    "                file_paths.append(filepath)  # Add it to the list.\n",
    "                # pdb.set_trace()\n",
    "    file_paths.sort()\n",
    "    return file_paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_file=get_filepaths('mixed_all_snr/')\n",
    "cleaned_file=get_filepaths('clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切割data and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mixed_file, cleaned_file, test_size=0.33, random_state=42)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Noisy_lists=X_train\n",
    "Train_Clean_paths= y_train\n",
    "\n",
    "Test_Noisy_lists  = X_test\n",
    "Test_Clean_paths = y_test\n",
    "          \n",
    "Num_testdata=len(Test_Noisy_lists)   \n",
    "Num_traindata=len(Train_Noisy_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.shape(noisy)[0] <88800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 0, 0, 0]), (6,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([0,0,0])\n",
    "\n",
    "x=np.hstack((a,b))\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, noisy = wavfile.read(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48449"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(noisy)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy=np.reshape(noisy,(1,np.shape(noisy)[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 48449, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(noisy_list, clean_path):\n",
    "    index=0\n",
    "    while True:\n",
    "        #noisy, rate  = librosa.load(noisy_list[index],sr=16000) \n",
    "        # print(noisy_list[index],index)\n",
    "        # pdb.set_trace()\n",
    "        rate, noisy = wavfile.read(noisy_list[index])\n",
    "        \n",
    "        # while noisy.shape[0]/16000.>7: # Audio length <7s or OOM \n",
    "        #     index += 1\n",
    "        #     rate, noisy = wavfile.read(noisy_list[index])\n",
    "\n",
    "        noisy=noisy.astype('float32')         \n",
    "        if len(noisy.shape)==2:\n",
    "            noisy=(noisy[:,0]+noisy[:,1])/2  \n",
    "        noisy=np.reshape(noisy,(1,np.shape(noisy)[0],1))\n",
    "\n",
    "        #clean, rate  =librosa.load(clean_list[clean_wav_list.index(noisy_wav_list[index])],sr=16000)         \n",
    "        rate, clean = wavfile.read(clean_path[index])\n",
    "        clean=clean.astype('float32')  \n",
    "        clean=clean/2**15\n",
    "            \n",
    "        clean=np.reshape(clean,(1,np.shape(clean)[0],1))\n",
    "        \n",
    "        index += 1\n",
    "        if index == len(noisy_list):\n",
    "            index = 0\n",
    "\n",
    "#             permute = list(range(len(noisy_list)))\n",
    "#             random.shuffle(permute)\n",
    "#             noisy_list=shuffle_list(noisy_list,permute)\n",
    "\n",
    "        yield noisy, clean\n",
    "\n",
    "def val_data_generator(noisy_list, clean_path):\n",
    "    index=0\n",
    "    while True:\n",
    "         #noisy, rate  = librosa.load(noisy_list[index],sr=16000)       \n",
    "        rate, noisy = wavfile.read(noisy_list[index])\n",
    "        noisy=noisy.astype('float32')         \n",
    "        if len(noisy.shape)==2:\n",
    "            noisy=(noisy[:,0]+noisy[:,1])/2       \n",
    "#         noisy=noisy/np.max(abs(noisy))\n",
    "#         if np.shape(noisy)[0] <88800:\n",
    "#             add_zeros = np.zeros(88800-np.shape(noisy)[0])\n",
    "#             noisy=np.hstack((noisy,add_zeros))\n",
    "#         noisy=np.reshape(noisy,(1,np.shape(noisy)[0],1))\n",
    "\n",
    "        noisy=np.reshape(noisy,(1,np.shape(noisy)[0],1))\n",
    "          \n",
    "        rate, clean = wavfile.read(clean_path[index])\n",
    "        clean=clean.astype('float32')  \n",
    "        if len(clean.shape)==2:\n",
    "            clean=(clean[:,0]+clean[:,1])/2\n",
    "        clean=clean/2**15\n",
    "#         if np.shape(clean)[0] <88800:\n",
    "#             add_zeros = np.zeros(88800-np.shape(noisy)[0])\n",
    "#             clean=np.hstack((clean,add_zeros))\n",
    "\n",
    "        clean=np.reshape(clean,(1,np.shape(clean)[0],1))\n",
    "        \n",
    "\n",
    "        index += 1\n",
    "        if index == len(noisy_list):\n",
    "            index = 0\n",
    "          \n",
    "        yield noisy, clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 09:05:57.425796 140509956875776 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/Keras-2.2.4-py3.7.egg/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 32)          4480      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, None, 32)          8448      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 1)           33        \n",
      "=================================================================\n",
      "Total params: 12,961\n",
      "Trainable params: 12,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(LSTM(32,return_sequences=True,input_shape=(None,1)))\n",
    "# model.add(LSTM(32,return_sequences=True)) # 返回维度为 32 的向量序列\n",
    "# model.add(Dense(1,activation='tanh'))\n",
    "\n",
    "model.add(CuDNNLSTM(32,return_sequences=True,input_shape=(None,1)))\n",
    "model.add(CuDNNLSTM(32,return_sequences=True)) # 返回维度为 32 的向量序列\n",
    "model.add(Dense(1,activation='tanh'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 09:05:59.311595 140509956875776 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/Keras-2.2.4-py3.7.egg/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=5, verbose=1, validation_data=<generator..., use_multiprocessing=True, steps_per_epoch=3859, validation_steps=1901, workers=16, max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/Keras-2.2.4-py3.7.egg/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3859/3859 [==============================] - 10888s 3s/step - loss: 2.2973e-04 - val_loss: 2.3801e-04\n",
      "Epoch 2/5\n",
      "3859/3859 [==============================] - 10530s 3s/step - loss: 1.9741e-04 - val_loss: 2.0186e-04\n",
      "Epoch 3/5\n",
      "1633/3859 [===========>..................] - ETA: 1:22:39 - loss: 1.6042e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3859/3859 [==============================] - 10476s 3s/step - loss: 2.2300e-04 - val_loss: 3.6632e-04\n",
      "Epoch 5/5\n",
      "1114/3859 [=======>......................] - ETA: 1:40:06 - loss: 2.5968e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=2\n",
    "batch_size=1\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "with open('{}.json'.format('firsttry'),'w') as f:    # save the model\n",
    "    f.write(model.to_json()) \n",
    "checkpointer = ModelCheckpoint(filepath='{}.hdf5'.format('firsttry'), verbose=1, save_best_only=True, mode='min')  \n",
    "\n",
    "print ('training...')\n",
    "\n",
    "g1 = train_data_generator(Train_Noisy_lists, Train_Clean_paths)\n",
    "print()\n",
    "g2 = val_data_generator(Test_Noisy_lists, Test_Clean_paths)\n",
    "\n",
    "tbCallBack = TensorBoard(log_dir='./logs',  # log 目录\n",
    "                 histogram_freq=0,  # 按照何等频率（epoch）来计算直方图，0为不计算\n",
    "#                  batch_size=32,     # 用多大量的数据计算直方图\n",
    "                 write_graph=True,  # 是否存储网络结构图\n",
    "                 write_grads=True, # 是否可视化梯度直方图\n",
    "                 write_images=True,# 是否可视化参数\n",
    "                 embeddings_freq=0, \n",
    "                 embeddings_layer_names=None, \n",
    "                 embeddings_metadata=None)   \n",
    "# hist=model.fit_generator(g1,steps_per_epoch=10000, epochs=epoch, verbose=1,\n",
    "#                             validation_data=g2,\n",
    "#                             validation_steps=g2,\n",
    "#                             max_queue_size=1, \n",
    "#                             workers=1,\n",
    "#                             )\n",
    "hist=model.fit_generator(g1,\n",
    "                         samples_per_epoch=Num_traindata,\n",
    "#                         steps_per_epoch=10000, \n",
    "                        epochs=epoch, \n",
    "                        verbose=1,\n",
    "                        validation_data=g2,\n",
    "                        nb_val_samples=Num_testdata,\n",
    "                        max_q_size=1, \n",
    "                        nb_worker=16,\n",
    "                        use_multiprocessing=True\n",
    "#                          pickle_safe=False,\n",
    "                         )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畫Loss圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@0.000214, Minimun error:0.000202, at iteration: 2\n",
      "The code for this file ran for 982.94m\n"
     ]
    }
   ],
   "source": [
    "# # plotting the learning curve\n",
    "TrainERR=hist.history['loss']\n",
    "ValidERR=hist.history['val_loss']\n",
    "print ('@%f, Minimun error:%f, at iteration: %i' % (hist.history['val_loss'][epoch-1], np.min(np.asarray(ValidERR)),np.argmin(np.asarray(ValidERR))+1))\n",
    "# print 'drawing the training process...'\n",
    "plt.figure(4)\n",
    "plt.plot(range(1,epoch+1),TrainERR,'b',label='TrainERR')\n",
    "plt.plot(range(1,epoch+1),ValidERR,'r',label='ValidERR')\n",
    "plt.xlim([1,epoch])\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig('Learning_curve_{}.png'.format('FCN_firsttry'), dpi=150)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print ('The code for this file ran for %.2fm' % ((end_time - start_time) / 60.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始predict囉 並轉成音檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxv = np.iinfo(np.int16).max \n",
    "for path in Test_Noisy_lists[:10]: # Ex: /mnt/hd-02/avse/testing/noisy/engine/1dB/1.wav\n",
    "    S=path.split('/') \n",
    "    # noise=S[-3]\n",
    "    # dB=S[-2]\n",
    "    wave_name=S[1]\n",
    "    \n",
    "    rate, noisy = wavfile.read(path)\n",
    "    noisy=noisy.astype('float32')\n",
    "    if len(noisy.shape)==2:\n",
    "        noisy=(noisy[:,0]+noisy[:,1])/2             \n",
    "#     noisy=noisy/np.max(abs(noisy))\n",
    "    noisy=np.reshape(noisy,(1,np.shape(noisy)[0],1))\n",
    "    enhanced=np.squeeze(model.predict(noisy, verbose=0, batch_size=batch_size))\n",
    "#     enhanced=enhanced/np.max(abs(enhanced))\n",
    "    enhanced=enhanced* maxv\n",
    "    enhanced_2=enhanced.astype('int16')\n",
    "#     enhanced=enhanced/2**15\n",
    "    wavfile.write(os.path.join(\"c\", wave_name),16000,(enhanced).astype(np.int16))\n",
    "#     librosa.output.write_wav(os.path.join(\"/FCN\", wave_name), (enhanced* maxv).astype(np.int16), 16000)\n",
    "\n",
    "#     librosa.output.write_wav(os.path.join(\"FCN_enhanced_MSE\",noise, dB, wave_name), (enhanced* maxv).astype(np.int16), 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
